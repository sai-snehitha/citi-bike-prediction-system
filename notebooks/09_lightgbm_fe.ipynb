{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Top 20 Selected Features:\n",
      " ['lag_1', 'lag_2', 'lag_3', 'lag_12', 'lag_13', 'lag_21', 'lag_22', 'lag_23', 'lag_24', 'lag_25', 'lag_26', 'lag_27', 'lag_35', 'lag_36', 'lag_37', 'lag_45', 'lag_46', 'lag_47', 'lag_48', 'hour']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Load the feature files for the top 3 locations\n",
    "locations = [\"HB102\", \"JC115\", \"HB105\"]\n",
    "dfs = []\n",
    "\n",
    "for loc in locations:\n",
    "    path = f\"../data/features/{loc}.csv\"  # <-- fixed path\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"location_id\"] = loc\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all data\n",
    "df_all = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "# Drop non-numeric columns before modeling\n",
    "X = df_all.drop(columns=[\"target\", \"pickup_hour\", \"location_id\"])\n",
    "y = df_all[\"target\"]\n",
    "\n",
    "# Select top 20 features\n",
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print(\"✅ Top 20 Selected Features:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (20880, 20) \n",
      "Test shape: (5220, 20)\n"
     ]
    }
   ],
   "source": [
    "# Use only top features for training\n",
    "X_top = df_all[selected_features]\n",
    "y = df_all[\"target\"]\n",
    "\n",
    "# Time-based train-test split (80% train, 20% test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"\\nTest shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1297\n",
      "[LightGBM] [Info] Number of data points in the train set: 20880, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 5.167720\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Train LightGBM model\n",
    "model = LGBMRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LightGBM (Top 20 Features) MAE: 1.867\n",
      "✅ LightGBM (Top 20 Features) RMSE: 2.7497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"✅ LightGBM (Top 20 Features) MAE:\", round(mae, 4))\n",
    "print(\"✅ LightGBM (Top 20 Features) RMSE:\", round(rmse, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LightGBM Model 3 MAE logged to DagsHub MLflow.\n",
      "🏃 View run Model 3 - LightGBM (Feature Selected) at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1/runs/c4ce4dd6d05a444682fe80d24887c06e\n",
      "🧪 View experiment at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# ✅ Set credentials (same as before)\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"sai-snehitha\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"546ba070a0d146826c8d49111843d556196bcf9a\"\n",
    "\n",
    "# ✅ Set MLflow tracking\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow\")\n",
    "mlflow.set_experiment(\"citi-bike-project\")\n",
    "\n",
    "# ✅ Log the MAE for Model 3 (replace with your actual result)\n",
    "with mlflow.start_run(run_name=\"Model 3 - LightGBM (Feature Selected)\"):\n",
    "    mlflow.log_metric(\"mae\", 1.867)  # 🔁 Replace this with your actual Model 3 MAE\n",
    "    print(\"✅ LightGBM Model 3 MAE logged to DagsHub MLflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced LGBM MAE for HB102: 1.8670\n",
      "🏃 View run Reduced_LGBM - HB102 at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1/runs/676106a4ce1640acbfde10fa582afa3c\n",
      "🧪 View experiment at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1\n",
      "Reduced LGBM MAE for HB105: 1.8670\n",
      "🏃 View run Reduced_LGBM - HB105 at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1/runs/8195120af318484a8ca4fec57fd3814d\n",
      "🧪 View experiment at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1\n",
      "Reduced LGBM MAE for JC115: 1.8670\n",
      "🏃 View run Reduced_LGBM - JC115 at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1/runs/e754832bfec34256be30fdcc78cda91c\n",
      "🧪 View experiment at: https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Set your DagsHub MLflow credentials\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"sai-snehitha\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"546ba070a0d146826c8d49111843d556196bcf9a\"  # Replace if needed\n",
    "\n",
    "# Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/sai-snehitha/citi-bike-prediction-system.mlflow\")\n",
    "mlflow.set_experiment(\"citi-bike-project\")\n",
    "\n",
    "# Loop through the top 3 locations\n",
    "for location in [\"HB102\", \"HB105\", \"JC115\"]:\n",
    "    # Assuming you've already selected features, trained model, made predictions, and computed MAE\n",
    "    # Example:\n",
    "    # X_train, X_test, y_train, y_test = ...\n",
    "    # model = ...\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Reduced LGBM MAE for {location}: {mae:.4f}\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Reduced_LGBM - {location}\"):\n",
    "        mlflow.set_tag(\"model_type\", \"Reduced_LGBM\")\n",
    "        mlflow.set_tag(\"location_id\", location)\n",
    "        mlflow.log_metric(\"mae\", mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
